"""In-memory state persistence for GraphAI.

This module provides simple in-memory persistence utilities inspired by
`pydantic-ai`'s state persistence, adapted to GraphAI's execution model.

Two implementations are provided:

- `SimpleStatePersistence`: keeps only the last snapshot
- `FullStatePersistence`: keeps an in-memory history of snapshots and supports
  JSON import/export

Both expose a common interface to snapshot the next node to run, record the end
result, and wrap node execution with a status+timing context manager.

Note: This module does not automatically integrate with `graphai.Graph`. You can
wire persistence into your runtime by calling `snapshot_node` just before invoking
the node and using `record_run(...)` around the invocation. See docstrings below
for a usage sketch.
"""

from __future__ import annotations
from abc import ABC, abstractmethod
import json
import copy
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime, timezone
from time import perf_counter
from typing import Any, AsyncIterator, Literal, AsyncContextManager

from pydantic import BaseModel, Field


NodeStatus = Literal["created", "pending", "running", "success", "error"]


def _now_utc() -> datetime:
    return datetime.now(timezone.utc)


def _node_name(obj: Any) -> str:
    name = getattr(obj, "name", None)
    return name if isinstance(name, str) else repr(obj)


class _SnapshotBase(BaseModel):
    """Base class for snapshots.

    Attributes:
        id:     A unique identifier for the snapshot instance. Autogenerated.
        status: Lifecycle status for execution bookkeeping.
        created_ts: Timestamp when the snapshot was created.
        start_ts:   Timestamp when the run started (set by `record_run`).
        duration:   Duration of the run in seconds (set by `record_run`).
    """

    id: str = Field(
        default_factory=lambda: f"snap-{datetime.now(timezone.utc).timestamp():.9f}"
    )
    status: NodeStatus = Field(default="created")
    created_ts: datetime = Field(default_factory=_now_utc)
    start_ts: datetime | None = None
    duration: float | None = None

    model_config = {
        "arbitrary_types_allowed": True,
        "extra": "forbid",
    }


class NodeSnapshot(_SnapshotBase):
    """Snapshot representing the next node to run and the graph state.

    - `node` is excluded from JSON to avoid serializing function/class objects.
      Use `node_name` to re-bind via a lookup if loading from JSON.
    """

    kind: Literal["node"] = Field(default="node")
    state: dict[str, Any] | None = None
    node_name: str
    # Avoid forward-ref to Graph's NodeProtocol so Pydantic doesn't need it
    node: Any | None = Field(default=None, exclude=True)


class EndSnapshot(_SnapshotBase):
    """Snapshot representing the end of a run and its result payload."""

    kind: Literal["end"] = Field(default="end")
    state: dict[str, Any] | None = None
    result: dict[str, Any] = Field(default_factory=dict)


Snapshot = NodeSnapshot | EndSnapshot


class BaseStatePersistence(ABC):
    """Abstract persistence interface for GraphAI.

    Methods are async to make it easy to adapt to non-memory backends.

    Example integration sketch (inside your execution loop):

        await persistence.snapshot_node(state=graph_state, next_node=current_node)
        sid = persistence.peek_last_id()
        async with persistence.record_run(sid):
            output = await current_node.invoke(input=state, state=graph_state)

        if is_end:
            await persistence.snapshot_end(state=graph_state, result=output)
    """

    @abstractmethod
    async def snapshot_node(self, state: dict[str, Any] | None, next_node: Any) -> None:
        raise NotImplementedError

    @abstractmethod
    async def snapshot_node_if_new(
        self, snapshot_id: str, state: dict[str, Any] | None, next_node: Any
    ) -> None:
        raise NotImplementedError

    @abstractmethod
    async def snapshot_end(
        self, state: dict[str, Any] | None, result: dict[str, Any]
    ) -> None:
        raise NotImplementedError

    @abstractmethod
    def record_run(self, snapshot_id: str) -> AsyncContextManager[None]:
        raise NotImplementedError

    @abstractmethod
    async def load_next(
        self,
    ) -> NodeSnapshot | None:
        raise NotImplementedError

    @abstractmethod
    async def load_all(self) -> list[Snapshot]:
        raise NotImplementedError

    @abstractmethod
    def peek_last_id(self) -> str | None:
        raise NotImplementedError


@dataclass
class SimpleStatePersistence(BaseStatePersistence):
    """Keeps only the most recent snapshot in memory.

    This mirrors the behavior of pydantic-ai's SimpleStatePersistence.
    """

    last_snapshot: Snapshot | None = None

    async def snapshot_node(self, state: dict[str, Any] | None, next_node: Any) -> None:
        self.last_snapshot = NodeSnapshot(
            state=state, node_name=_node_name(next_node), node=next_node
        )

    async def snapshot_node_if_new(
        self, snapshot_id: str, state: dict[str, Any] | None, next_node: Any
    ) -> None:
        if self.last_snapshot and self.last_snapshot.id == snapshot_id:
            return  # existing last snapshot already matches
        await self.snapshot_node(state, next_node)

    async def snapshot_end(
        self, state: dict[str, Any] | None, result: dict[str, Any]
    ) -> None:
        self.last_snapshot = EndSnapshot(state=state, result=result)

    @asynccontextmanager
    async def record_run(self, snapshot_id: str) -> AsyncIterator[None]:
        if self.last_snapshot is None or snapshot_id != self.last_snapshot.id:
            raise LookupError(f"No snapshot found with id={snapshot_id!r}")

        if not isinstance(self.last_snapshot, NodeSnapshot):
            raise AssertionError("Only NodeSnapshot can be recorded")
        if self.last_snapshot.status not in ("created", "pending"):
            raise ValueError(
                f"Invalid snapshot status for record_run: {self.last_snapshot.status}"
            )

        self.last_snapshot.status = "running"
        self.last_snapshot.start_ts = _now_utc()

        start = perf_counter()
        try:
            yield
        except Exception:
            self.last_snapshot.duration = perf_counter() - start
            self.last_snapshot.status = "error"
            raise
        else:
            self.last_snapshot.duration = perf_counter() - start
            self.last_snapshot.status = "success"

    async def load_next(self) -> NodeSnapshot | None:
        if (
            isinstance(self.last_snapshot, NodeSnapshot)
            and self.last_snapshot.status == "created"
        ):
            self.last_snapshot.status = "pending"
            return self.last_snapshot
        return None

    async def load_all(
        self,
    ) -> list[Snapshot]:  # pragma: no cover - parity with example (unsupported)
        raise NotImplementedError(
            "load_all is not supported for SimpleStatePersistence"
        )

    def peek_last_id(self) -> str | None:
        return None if self.last_snapshot is None else self.last_snapshot.id


@dataclass
class FullStatePersistence(BaseStatePersistence):
    """In-memory persistence that keeps a full history of snapshots.

    - `deep_copy` applies to `state` and `result` payloads (not to the node object)
      so the recorded history reflects values at the time of snapshot.
    - The `node` reference is excluded from JSON; only `node_name` is serialized.
      Use `rebind_nodes` after `load_json` to hydrate `node` from a name->node map.
    """

    deep_copy: bool = True
    history: list[Snapshot] | None = None

    def __post_init__(self) -> None:
        if self.history is None:
            self.history = []

    def _prep_state(self, state: dict[str, Any] | None) -> dict[str, Any] | None:
        if not self.deep_copy or state is None:
            return state
        return copy.deepcopy(state)

    def _prep_result(self, result: dict[str, Any]) -> dict[str, Any]:
        if not self.deep_copy:
            return result
        return copy.deepcopy(result)

    async def snapshot_node(self, state: dict[str, Any] | None, next_node: Any) -> None:
        snap = NodeSnapshot(
            state=self._prep_state(state),
            node_name=_node_name(next_node),
            node=next_node,
        )
        assert self.history is not None
        self.history.append(snap)

    async def snapshot_node_if_new(
        self, snapshot_id: str, state: dict[str, Any] | None, next_node: Any
    ) -> None:
        assert self.history is not None
        if not any(s.id == snapshot_id for s in self.history):
            await self.snapshot_node(state, next_node)

    async def snapshot_end(
        self, state: dict[str, Any] | None, result: dict[str, Any]
    ) -> None:
        snap = EndSnapshot(
            state=self._prep_state(state), result=self._prep_result(result)
        )
        assert self.history is not None
        self.history.append(snap)

    @asynccontextmanager
    async def record_run(self, snapshot_id: str) -> AsyncIterator[None]:
        assert self.history is not None
        snapshot: Snapshot | None = next(
            (s for s in self.history if s.id == snapshot_id), None
        )
        if snapshot is None:
            raise LookupError(f"No snapshot found with id={snapshot_id!r}")
        if not isinstance(snapshot, NodeSnapshot):
            raise AssertionError("Only NodeSnapshot can be recorded")
        if snapshot.status not in ("created", "pending"):
            raise ValueError(
                f"Invalid snapshot status for record_run: {snapshot.status}"
            )

        snapshot.status = "running"
        snapshot.start_ts = _now_utc()
        start = perf_counter()
        try:
            yield
        except Exception:
            snapshot.duration = perf_counter() - start
            snapshot.status = "error"
            raise
        else:
            snapshot.duration = perf_counter() - start
            snapshot.status = "success"

    async def load_next(self) -> NodeSnapshot | None:
        assert self.history is not None
        for s in self.history:
            if isinstance(s, NodeSnapshot) and s.status == "created":
                s.status = "pending"
                return s
        return None

    async def load_all(self) -> list[Snapshot]:
        assert self.history is not None
        return self.history

    def peek_last_id(self) -> str:
        assert self.history is not None
        return self.history[-1].id

    def dump_json(self) -> bytes:
        assert self.history is not None
        payload = [s.model_dump() for s in self.history]
        return json.dumps(payload, indent=2, default=str).encode("utf-8")

    def load_json(self, json_data: str | bytes | bytearray) -> None:
        data = json.loads(json_data)
        if not isinstance(data, list):
            raise TypeError("Expected a list of snapshots in JSON payload")
        history: list[Snapshot] = []
        for item in data:
            if not isinstance(item, dict):
                raise TypeError("Invalid snapshot record (expected object)")
            kind = item.get("kind")
            if kind == "node":
                history.append(NodeSnapshot(**item))
            elif kind == "end":
                history.append(EndSnapshot(**item))
            else:
                raise ValueError(f"Unknown snapshot kind: {kind!r}")
        self.history = history

    def rebind_nodes(self, name_lookup: dict[str, Any]) -> None:
        """Hydrate `node` references for node snapshots by name.

        Call after `load_json` with a mapping of node names to node objects
        (e.g. from a compiled graph instance).
        """

        assert self.history is not None
        for s in self.history:
            if isinstance(s, NodeSnapshot):
                s.node = name_lookup.get(s.node_name)
